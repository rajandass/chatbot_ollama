"use strict";
Object.defineProperty(exports, "__esModule", { value: true });
exports.ChatOllama = void 0;
const messages_1 = require("@langchain/core/messages");
const chat_models_1 = require("@langchain/core/language_models/chat_models");
const browser_1 = require("ollama/browser");
const outputs_1 = require("@langchain/core/outputs");
const messages_2 = require("@langchain/core/messages");
const function_calling_1 = require("@langchain/core/utils/function_calling");
const stream_1 = require("@langchain/core/utils/stream");
const utils_js_1 = require("./utils.cjs");
/**
 * Integration with the Ollama SDK.
 *
 * @example
 * ```typescript
 * import { ChatOllama } from "@langchain/ollama";
 *
 * const model = new ChatOllama({
 *   model: "llama3", // Default model.
 * });
 *
 * const result = await model.invoke([
 *   "human",
 *   "What is a good name for a company that makes colorful socks?",
 * ]);
 * console.log(result);
 * ```
 */
class ChatOllama extends chat_models_1.BaseChatModel {
    // Used for tracing, replace with the same name as your class
    static lc_name() {
        return "ChatOllama";
    }
    constructor(fields) {
        super(fields ?? {});
        Object.defineProperty(this, "model", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: "llama3"
        });
        Object.defineProperty(this, "numa", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        Object.defineProperty(this, "numCtx", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        Object.defineProperty(this, "numBatch", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        Object.defineProperty(this, "numGpu", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        Object.defineProperty(this, "mainGpu", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        Object.defineProperty(this, "lowVram", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        Object.defineProperty(this, "f16Kv", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        Object.defineProperty(this, "logitsAll", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        Object.defineProperty(this, "vocabOnly", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        Object.defineProperty(this, "useMmap", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        Object.defineProperty(this, "useMlock", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        Object.defineProperty(this, "embeddingOnly", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        Object.defineProperty(this, "numThread", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        Object.defineProperty(this, "numKeep", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        Object.defineProperty(this, "seed", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        Object.defineProperty(this, "numPredict", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        Object.defineProperty(this, "topK", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        Object.defineProperty(this, "topP", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        Object.defineProperty(this, "tfsZ", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        Object.defineProperty(this, "typicalP", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        Object.defineProperty(this, "repeatLastN", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        Object.defineProperty(this, "temperature", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        Object.defineProperty(this, "repeatPenalty", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        Object.defineProperty(this, "presencePenalty", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        Object.defineProperty(this, "frequencyPenalty", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        Object.defineProperty(this, "mirostat", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        Object.defineProperty(this, "mirostatTau", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        Object.defineProperty(this, "mirostatEta", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        Object.defineProperty(this, "penalizeNewline", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        Object.defineProperty(this, "streaming", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        Object.defineProperty(this, "format", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        Object.defineProperty(this, "keepAlive", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: "5m"
        });
        Object.defineProperty(this, "client", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        Object.defineProperty(this, "checkOrPullModel", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: false
        });
        Object.defineProperty(this, "baseUrl", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: "http://127.0.0.1:11434"
        });
        this.client = new browser_1.Ollama({
            host: fields?.baseUrl,
        });
        this.baseUrl = fields?.baseUrl ?? this.baseUrl;
        this.model = fields?.model ?? this.model;
        this.numa = fields?.numa;
        this.numCtx = fields?.numCtx;
        this.numBatch = fields?.numBatch;
        this.numGpu = fields?.numGpu;
        this.mainGpu = fields?.mainGpu;
        this.lowVram = fields?.lowVram;
        this.f16Kv = fields?.f16Kv;
        this.logitsAll = fields?.logitsAll;
        this.vocabOnly = fields?.vocabOnly;
        this.useMmap = fields?.useMmap;
        this.useMlock = fields?.useMlock;
        this.embeddingOnly = fields?.embeddingOnly;
        this.numThread = fields?.numThread;
        this.numKeep = fields?.numKeep;
        this.seed = fields?.seed;
        this.numPredict = fields?.numPredict;
        this.topK = fields?.topK;
        this.topP = fields?.topP;
        this.tfsZ = fields?.tfsZ;
        this.typicalP = fields?.typicalP;
        this.repeatLastN = fields?.repeatLastN;
        this.temperature = fields?.temperature;
        this.repeatPenalty = fields?.repeatPenalty;
        this.presencePenalty = fields?.presencePenalty;
        this.frequencyPenalty = fields?.frequencyPenalty;
        this.mirostat = fields?.mirostat;
        this.mirostatTau = fields?.mirostatTau;
        this.mirostatEta = fields?.mirostatEta;
        this.penalizeNewline = fields?.penalizeNewline;
        this.streaming = fields?.streaming;
        this.format = fields?.format;
        this.keepAlive = fields?.keepAlive ?? this.keepAlive;
        this.checkOrPullModel = fields?.checkOrPullModel ?? this.checkOrPullModel;
    }
    // Replace
    _llmType() {
        return "ollama";
    }
    /**
     * Download a model onto the local machine.
     *
     * @param {string} model The name of the model to download.
     * @param {PullModelOptions | undefined} options Options for pulling the model.
     * @returns {Promise<void>}
     */
    async pull(model, options) {
        const { stream, insecure, logProgress } = {
            stream: true,
            ...options,
        };
        if (stream) {
            for await (const chunk of await this.client.pull({
                model,
                insecure,
                stream,
            })) {
                if (logProgress) {
                    console.log(chunk);
                }
            }
        }
        else {
            const response = await this.client.pull({ model, insecure });
            if (logProgress) {
                console.log(response);
            }
        }
    }
    bindTools(tools, kwargs) {
        return this.bind({
            tools: tools.map(function_calling_1.convertToOpenAITool),
            ...kwargs,
        });
    }
    getLsParams(options) {
        const params = this.invocationParams(options);
        return {
            ls_provider: "ollama",
            ls_model_name: this.model,
            ls_model_type: "chat",
            ls_temperature: params.options?.temperature ?? undefined,
            ls_max_tokens: params.options?.num_predict ?? undefined,
            ls_stop: options.stop,
        };
    }
    invocationParams(options) {
        if (options?.tool_choice) {
            throw new Error("Tool choice is not supported for ChatOllama.");
        }
        return {
            model: this.model,
            format: this.format,
            keep_alive: this.keepAlive,
            options: {
                numa: this.numa,
                num_ctx: this.numCtx,
                num_batch: this.numBatch,
                num_gpu: this.numGpu,
                main_gpu: this.mainGpu,
                low_vram: this.lowVram,
                f16_kv: this.f16Kv,
                logits_all: this.logitsAll,
                vocab_only: this.vocabOnly,
                use_mmap: this.useMmap,
                use_mlock: this.useMlock,
                embedding_only: this.embeddingOnly,
                num_thread: this.numThread,
                num_keep: this.numKeep,
                seed: this.seed,
                num_predict: this.numPredict,
                top_k: this.topK,
                top_p: this.topP,
                tfs_z: this.tfsZ,
                typical_p: this.typicalP,
                repeat_last_n: this.repeatLastN,
                temperature: this.temperature,
                repeat_penalty: this.repeatPenalty,
                presence_penalty: this.presencePenalty,
                frequency_penalty: this.frequencyPenalty,
                mirostat: this.mirostat,
                mirostat_tau: this.mirostatTau,
                mirostat_eta: this.mirostatEta,
                penalize_newline: this.penalizeNewline,
                stop: options?.stop,
            },
            tools: options?.tools?.length
                ? options.tools.map(function_calling_1.convertToOpenAITool)
                : undefined,
        };
    }
    /**
     * Check if a model exists on the local machine.
     *
     * @param {string} model The name of the model to check.
     * @returns {Promise<boolean>} Whether or not the model exists.
     */
    async checkModelExistsOnMachine(model) {
        const { models } = await this.client.list();
        return !!models.find((m) => m.name === model || m.name === `${model}:latest`);
    }
    async _generate(messages, options, runManager) {
        if (this.checkOrPullModel) {
            if (!(await this.checkModelExistsOnMachine(this.model))) {
                await this.pull(this.model, {
                    logProgress: true,
                });
            }
        }
        let finalChunk;
        for await (const chunk of this._streamResponseChunks(messages, options, runManager)) {
            if (!finalChunk) {
                finalChunk = chunk.message;
            }
            else {
                finalChunk = (0, stream_1.concat)(finalChunk, chunk.message);
            }
        }
        // Convert from AIMessageChunk to AIMessage since `generate` expects AIMessage.
        const nonChunkMessage = new messages_1.AIMessage({
            id: finalChunk?.id,
            content: finalChunk?.content ?? "",
            tool_calls: finalChunk?.tool_calls,
            response_metadata: finalChunk?.response_metadata,
            usage_metadata: finalChunk?.usage_metadata,
        });
        return {
            generations: [
                {
                    text: typeof nonChunkMessage.content === "string"
                        ? nonChunkMessage.content
                        : "",
                    message: nonChunkMessage,
                },
            ],
        };
    }
    /**
     * Implement to support streaming.
     * Should yield chunks iteratively.
     */
    async *_streamResponseChunks(messages, options, runManager) {
        if (this.checkOrPullModel) {
            if (!(await this.checkModelExistsOnMachine(this.model))) {
                await this.pull(this.model, {
                    logProgress: true,
                });
            }
        }
        const params = this.invocationParams(options);
        // TODO: remove cast after SDK adds support for tool calls
        const ollamaMessages = (0, utils_js_1.convertToOllamaMessages)(messages);
        const usageMetadata = {
            input_tokens: 0,
            output_tokens: 0,
            total_tokens: 0,
        };
        if (params.tools && params.tools.length > 0) {
            const toolResult = await this.client.chat({
                ...params,
                messages: ollamaMessages,
                stream: false, // Ollama currently does not support streaming with tools
            });
            const { message: responseMessage, ...rest } = toolResult;
            usageMetadata.input_tokens += rest.prompt_eval_count ?? 0;
            usageMetadata.output_tokens += rest.eval_count ?? 0;
            usageMetadata.total_tokens =
                usageMetadata.input_tokens + usageMetadata.output_tokens;
            yield new outputs_1.ChatGenerationChunk({
                text: responseMessage.content,
                message: (0, utils_js_1.convertOllamaMessagesToLangChain)(responseMessage, {
                    responseMetadata: rest,
                    usageMetadata,
                }),
            });
            return runManager?.handleLLMNewToken(responseMessage.content);
        }
        const stream = await this.client.chat({
            ...params,
            messages: ollamaMessages,
            stream: true,
        });
        let lastMetadata;
        for await (const chunk of stream) {
            if (options.signal?.aborted) {
                this.client.abort();
            }
            const { message: responseMessage, ...rest } = chunk;
            usageMetadata.input_tokens += rest.prompt_eval_count ?? 0;
            usageMetadata.output_tokens += rest.eval_count ?? 0;
            usageMetadata.total_tokens =
                usageMetadata.input_tokens + usageMetadata.output_tokens;
            lastMetadata = rest;
            yield new outputs_1.ChatGenerationChunk({
                text: responseMessage.content ?? "",
                message: (0, utils_js_1.convertOllamaMessagesToLangChain)(responseMessage),
            });
            await runManager?.handleLLMNewToken(responseMessage.content ?? "");
        }
        // Yield the `response_metadata` as the final chunk.
        yield new outputs_1.ChatGenerationChunk({
            text: "",
            message: new messages_2.AIMessageChunk({
                content: "",
                response_metadata: lastMetadata,
                usage_metadata: usageMetadata,
            }),
        });
    }
}
exports.ChatOllama = ChatOllama;
